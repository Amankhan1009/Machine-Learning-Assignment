{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77b4647-e92a-4e10-b73b-8f4f672e3e5f",
   "metadata": {},
   "source": [
    "Q1.What is unsupervised learning in the context of machine learning?\n",
    "\n",
    "Unsupervised learning is a type of machine learning where the algorithm learns patterns and structures from unlabeled data. It identifies inherent relationships, clusters, or structures in the data without explicit supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1de534-c466-44e4-9563-a78b784510d2",
   "metadata": {},
   "source": [
    "Q2.How does the K-Means clustering algorithm work?\n",
    "\n",
    "\n",
    "K-Means clustering works by:\n",
    "\n",
    "Selecting \n",
    "ùêæ\n",
    "K initial cluster centroids randomly.\n",
    "\n",
    "Assigning each data point to the nearest centroid.\n",
    "\n",
    "Updating centroids by computing the mean of all points in a cluster.\n",
    "\n",
    "Repeating the process until centroids no longer change significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf0efb-ba24-443c-9ffb-c90ba7f935dd",
   "metadata": {},
   "source": [
    "Q3.Explain the concept of a dendrogram in hierarchical clustering.\n",
    "\n",
    "A dendrogram is a tree-like diagram that shows how clusters are merged or split at different levels in hierarchical clustering. It helps visualize the clustering hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546012d7-b13e-4eaa-ac62-678fc704db05",
   "metadata": {},
   "source": [
    "Q4.What is the main difference between K-Means and Hierarchical Clustering?\n",
    "\n",
    "K-Means requires a predefined number of clusters; hierarchical clustering does not.\n",
    "\n",
    "Hierarchical clustering creates a tree-based structure (dendrogram), while K-Means partitions data into fixed clusters.\n",
    "\n",
    "K-Means is more efficient on large datasets, while hierarchical clustering is computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa2868f-88c6-455a-8e71-e3aecd4e6fbc",
   "metadata": {},
   "source": [
    "Q5.What are the advantages of DBSCAN over K-Means?\n",
    "\n",
    "Can find clusters of arbitrary shape.\n",
    "\n",
    "Does not require the number of clusters to be predefined.\n",
    "\n",
    "Can identify noise points (outliers).\n",
    "\n",
    "Works well with clusters of varying density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ec13e-fa61-4d1a-bd76-8a18d1ba6427",
   "metadata": {},
   "source": [
    "Q6.When would you use Silhouette Score in clustering?\n",
    "\n",
    "Silhouette Score is used to evaluate clustering quality by measuring how similar a data point is to its own cluster compared to other clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f45049-5f3a-4a24-92f1-b3f405da1cf1",
   "metadata": {},
   "source": [
    "Q7.What are the limitations of Hierarchical Clustering?\n",
    "\n",
    "High computational complexity (\n",
    "ùëÇ\n",
    "(\n",
    "ùëõ\n",
    "3\n",
    ")\n",
    ")\n",
    "\n",
    "\n",
    "Sensitive to noise and outliers.\n",
    "\n",
    "Hard to handle large datasets.\n",
    "\n",
    "Once a merge/split is done, it cannot be undone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0ec874-9875-4304-bf02-cd5b73d25f1e",
   "metadata": {},
   "source": [
    "Q8.Why is feature scaling important in clustering algorithms like K-Means?\n",
    "\n",
    "Clustering algorithms rely on distance metrics, and features with larger numerical ranges dominate those with smaller ranges. Scaling ensures all features contribute equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e2798-656c-4a02-bca2-a64088c357f0",
   "metadata": {},
   "source": [
    "Q9.How does DBSCAN identify noise points?\n",
    "\n",
    "Points that do not have enough neighboring points within a specified radius (\n",
    "ùúñ) are classified as noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad1e2c9-e29f-4408-8680-2599babf2a9b",
   "metadata": {},
   "source": [
    "Q10.Define inertia in the context of K-Means.\n",
    "\n",
    "Inertia is the sum of squared distances of each point from its nearest cluster centroid. It represents the compactness of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21539e77-0a0b-44f5-b4a0-e046b6f1baed",
   "metadata": {},
   "source": [
    "Q11.What is the elbow method in K-Means clustering?\n",
    "\n",
    "The elbow method helps find the optimal number of clusters by plotting inertia vs. \n",
    "ùêæ\n",
    "K and identifying the \"elbow point\" where inertia stops decreasing significantly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9a3f2b-28a1-48db-a0bc-c90bba679308",
   "metadata": {},
   "source": [
    "Q12.Describe the concept of \"density\" in DBSCAN.\n",
    "\n",
    "Density in DBSCAN refers to the number of points within a specified radius (\n",
    "ùúñ\n",
    "œµ). A cluster consists of core points with enough density (neighbors) and border points connected to core points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baae2d9-fdd0-48d3-9516-19e16a0de016",
   "metadata": {},
   "source": [
    "Q13.Can hierarchical clustering be used on categorical data?\n",
    "\n",
    "Yes, but it requires appropriate distance measures like Hamming distance or Jaccard similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8bf31b-9b73-418f-b27f-3711dbe99501",
   "metadata": {},
   "source": [
    "Q14.What does a negative Silhouette Score indicate?\n",
    "\n",
    "A negative Silhouette Score means a data point is likely misclassified and assigned to the wrong cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ad107-a3b7-497e-aabd-22ffd407d763",
   "metadata": {},
   "source": [
    "Q15.Explain the term \"linkage criteria\" in hierarchical clustering.\n",
    "\n",
    "Linkage criteria determine how distances between clusters are measured when merging clusters. Common types:\n",
    "\n",
    "- Single linkage: minimum distance between points.\n",
    "\n",
    "- Complete linkage: maximum distance.\n",
    "\n",
    "- Average linkage: mean distance.\n",
    "\n",
    "- Ward‚Äôs method: minimizes variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863569e0-934c-4297-a3dc-75c11268a384",
   "metadata": {},
   "source": [
    "Q16.Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n",
    "\n",
    "K-Means assumes clusters are spherical and of similar size. It struggles with clusters of varying densities, non-convex shapes, or different sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7a0761-da9e-4908-ac6f-420a2fdd6b4b",
   "metadata": {},
   "source": [
    "Q17.What are the core parameters in DBSCAN, and how do they influence clustering?\n",
    "\n",
    "\n",
    "- œµ (epsilon): defines the neighborhood radius.\n",
    "\n",
    "- minPts (minimum points): defines the minimum number of points required to form a cluster.\n",
    "\n",
    "- \n",
    "Higher \n",
    "œµ may merge clusters, while lower \n",
    "minPts\n",
    "minPts may detect noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760a822-042f-4048-9c24-8d0b9da317d5",
   "metadata": {},
   "source": [
    "Q18.How does K-Means++ improve upon standard K-Means initialization?\n",
    "\n",
    "K-Means++ selects initial centroids in a way that maximizes the distance between them, reducing the risk of poor clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b98327e-183b-452e-b444-205c586439b2",
   "metadata": {},
   "source": [
    "Q19.What is agglomerative clustering?\n",
    "\n",
    "Agglomerative clustering is a bottom-up approach to hierarchical clustering where each data point starts as its own cluster and iteratively merges with the closest cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85950d79-7c9e-4230-913c-a61557190086",
   "metadata": {},
   "source": [
    "Q20.What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
    "\n",
    "- Inertia only measures compactness, while Silhouette Score considers both compactness and separation between clusters.\n",
    "\n",
    "- It works with different clustering algorithms, unlike inertia which is specific to K-Means."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
