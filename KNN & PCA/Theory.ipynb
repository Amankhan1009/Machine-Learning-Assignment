{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.What is K-Nearest Neighbors (KNN) and how does it work?\n",
    "\n",
    "KNN is a supervised learning algorithm used for classification and regression. It works by finding the 'K' nearest data points to a given query point and assigning the most common class (for classification) or the average of values (for regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.What is the difference between KNN Classification and KNN Regression?\n",
    "\n",
    "KNN Classification: Predicts the class label based on the majority of the 'K' nearest neighbors.\n",
    "\n",
    "KNN Regression: Predicts a numerical value by averaging the values of 'K' nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.What is the role of the distance metric in KNN?\n",
    "\n",
    "The distance metric (e.g., Euclidean, Manhattan, Minkowski) determines the closeness of data points. Choosing the right metric is crucial for accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4.What is the Curse of Dimensionality in KNN?\n",
    "\n",
    "As the number of features (dimensions) increases, the distance between points becomes less meaningful, making KNN less effective due to sparsity in high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5.How can we choose the best value of K in KNN?\n",
    "\n",
    "The best value of K is typically found using cross-validation. A small K may lead to overfitting, while a large K may cause underfitting. A common approach is to test different values and choose the one with the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6.What are KD Tree and Ball Tree in KNN?\n",
    "\n",
    "KD Tree and Ball Tree are data structures used to speed up nearest neighbor searches. KD Tree is useful for low-dimensional data, while Ball Tree is better for high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7.When should you use KD Tree vs. Ball Tree?\n",
    "\n",
    "KD Tree: Suitable for low-dimensional data (up to ~20 dimensions).\n",
    "\n",
    "Ball Tree: Works better for high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8.What are the disadvantages of KNN?\n",
    "\n",
    "Computationally expensive for large datasets.\n",
    "\n",
    "Sensitive to irrelevant features and the choice of distance metric.\n",
    "\n",
    "Affected by imbalanced data distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9.How does feature scaling affect KNN?\n",
    "\n",
    "KNN is distance-based, so features with larger magnitudes can dominate the distance calculation. Scaling (e.g., Min-Max Scaling, Standardization) ensures all features contribute equally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10.What is PCA (Principal Component Analysis)?\n",
    "\n",
    "PCA is a dimensionality reduction technique that transforms correlated features into a smaller set of uncorrelated components called principal components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11.How does PCA work?\n",
    "\n",
    "PCA works by computing the covariance matrix, finding eigenvalues and eigenvectors, and projecting data onto the top principal components to reduce dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12.What is the geometric intuition behind PCA?\n",
    "\n",
    "PCA finds new axes (principal components) that capture the maximum variance in the data while being orthogonal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q13.What is the difference between Feature Selection and Feature Extraction?\n",
    "\n",
    "Feature Selection: Chooses a subset of existing features.\n",
    "\n",
    "Feature Extraction: Creates new transformed features (e.g., PCA generates principal components).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q14.What are Eigenvalues and Eigenvectors in PCA?\n",
    "\n",
    "Eigenvectors define the new principal component directions, and eigenvalues represent the variance captured by each eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q15.How do you decide the number of components to keep in PCA?\n",
    "\n",
    "By analyzing the explained variance ratio, a common approach is to keep enough components to capture 95-99% of the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q16.Can PCA be used for classification?\n",
    "\n",
    "PCA is primarily for dimensionality reduction, but after reducing features, the transformed data can be used for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q17.What are the limitations of PCA?\n",
    "\n",
    "Assumes linear relationships in data.\n",
    "\n",
    "Difficult to interpret transformed features.\n",
    "\n",
    "Sensitive to scaling; improper scaling can affect results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q18.How do KNN and PCA complement each other?\n",
    "\n",
    "PCA reduces dimensions to mitigate the curse of dimensionality, improving KNN's efficiency and performance in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q19.How does KNN handle missing values in a dataset?\n",
    "\n",
    "KNN can impute missing values by averaging (for numerical data) or selecting the most frequent class (for categorical data) among the nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q20.What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n",
    "\n",
    "PCA maximizes variance without considering class labels, while LDA finds directions that best separate different classes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
